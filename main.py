import argparse
import os
import git
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline
from typing import List

# Constants for the LLM
SYSTEM_PROMPT = "You are an assistant that generates concise and informative git commit messages based on provided git diffs."

# Retry and rate limiting settings
MAX_RETRIES = 3  # Number of retries for the LLM call
BASE_DELAY = 1   # Base delay in seconds between retries

def is_git_repository(path: str) -> bool:
    try:
        _ = git.Repo(path).git_dir
        return True
    except git.exc.InvalidGitRepositoryError:
        return False

def get_git_diffs(repo_path: str) -> str:
    """
    Get the git diffs for the repository.

    Args:
        repo_path (str): The path to the git repository.

    Returns:
        str: A string containing the diffs.
    """
    repo = git.Repo(repo_path)
    diffs = []
    for item in repo.index.diff(None):
        diff_text = repo.git.diff(item.a_path)
        # Remove lines that don't start with '+' or '-'
        diff_text_filtered = "\n".join(
            [
                line
                for line in diff_text.split("\n")
                if line.startswith("+") or line.startswith("-")
            ]
        )
        if diff_text_filtered.strip():
            diffs.append(f"File: {item.a_path}\nChanges:\n{diff_text_filtered}\n")
    return "\n".join(diffs)

def generate_commit_message(diffs: str, pipeline: TextGenerationPipeline) -> str | None:
    """
    Generates a commit message based on the git diffs by calling the LLM.

    Args:
        diffs (str): The git diffs to use for generating the commit message.
        pipeline (TextGenerationPipeline): The text generation pipeline using Mistral.

    Returns:
        str | None: The generated commit message, or None if it could not be generated.
    """
    # Prepare the prompt for the LLM
    prompt = f"""
Given the following git diffs, generate a concise and informative commit message:

{diffs}

Commit message:
"""
    # Call the LLM
    commit_message = call_llm(prompt, pipeline)
    return commit_message

def call_llm(prompt: str, pipeline: TextGenerationPipeline) -> str | None:
    """
    Calls the Mistral model to generate a commit message based on the provided prompt.

    Implements retries with exponential backoff in case of failures.

    Args:
        prompt (str): The prompt to send to the LLM.
        pipeline (TextGenerationPipeline): The text generation pipeline using Mistral.

    Returns:
        str | None: The generated commit message, or None if the call failed.
    """
    for attempt in range(MAX_RETRIES):
        try:
            # Generate the response
            response = pipeline(
                prompt,
                max_length=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.95,
                num_return_sequences=1,
                eos_token_id=pipeline.tokenizer.eos_token_id,
            )
            if response:
                generated_text = response[0]['generated_text']
                # Extract the commit message from the generated text
                commit_message = extract_commit_message(generated_text)
                return commit_message.strip()
            else:
                print("No response from the LLM.")
        except Exception as e:
            print(f"An error occurred during the LLM call: {str(e)}")

        # Exponential backoff
        delay = BASE_DELAY * (2 ** attempt)
        print(f"Retrying in {delay} seconds...")
        time.sleep(delay)

    # If all retries failed, return None
    return None

def extract_commit_message(generated_text: str) -> str:
    """
    Extracts the commit message from the generated text.

    Args:
        generated_text (str): The text generated by the LLM.

    Returns:
        str: The extracted commit message.
    """
    # Assuming the commit message follows "Commit message:"
    split_text = generated_text.split("Commit message:")
    if len(split_text) > 1:
        return split_text[1]
    else:
        # If the LLM did not follow the expected format
        return generated_text

def main():
    parser = argparse.ArgumentParser(
        description="Generate commit message suggestions using an LLM."
    )
    parser.add_argument("repo_path", help="Path to the git repository")
    args = parser.parse_args()

    if not is_git_repository(args.repo_path):
        print("Error: The specified path is not a valid git repository.")
        return

    diffs = get_git_diffs(args.repo_path)
    if not diffs.strip():
        print("No changes detected in the repository.")
        return

    try:
        # Load the Mistral model and tokenizer
        print("Loading the Mistral model. This may take a few minutes...")
        tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(
            "mistralai/Mistral-7B-v0.1",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )

        # Create the text generation pipeline
        pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)

        commit_message = generate_commit_message(diffs, pipeline)
        if commit_message is None:
            print("No commit message generated.")
            return

        print("Suggested commit message:")
        print(commit_message)
    except Exception as e:
        print(f"An error occurred while generating the commit message: {str(e)}")
        print("Please try again later or write the commit message manually.")

if __name__ == "__main__":
    main()
